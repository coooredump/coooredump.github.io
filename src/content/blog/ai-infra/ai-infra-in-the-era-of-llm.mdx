---
title: LLM 黄金时代下的 AI Infra
description: AI Infra 是连接算力和应用的 AI 中间层基础设施，涵盖了数据准备、模型训练、模型部署和应用整合等环节，其中的基础软件工具有较高商业化潜力；目前 AI Infra 产业处于高速增长的发展早期，未来几年内各细分赛道有望保持 30%+ 高速增长。
publishDate: 2025-07-18
tags: ['ai infra']
language: "Chinese"
comment: true
draft: false
---

类比计算机系统的基础软件层以及云计算三层架构的 PaaS 层级，我们认为，AI 产业链中也有层级相似，定位于算力与应用之间的“桥梁”角色的基础软件设施层即 AI Infra。新一轮生成式 AI 浪潮，对于上层应用而言机遇与挑战并存，而 AI Infra 作为必要的基础设施，我们认为其技术及商业发展前景的确定性或更强。本文我们聚焦 AI Infra，揭示其内涵并总结目前国内外项目的商业化进展，再从工作流视角详细梳理各环节及代表厂商。**我们认为，AI Infra 是 AI 产业必不可少的基础软件堆栈，“掘金卖铲”逻辑强、商业潜质高，建议投资者持续关注 AI Infra 相关投资机会**。

![AI 产业链](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181309148.jpeg)

## 摘要

在预训练大模型时代，我们可以从应用落地过程里提炼出标准化的工作流，AI Infra 的投资机会得以演绎。传统 ML 时代 AI 模型通用性较低，项目落地停留在“手工作坊”阶段，流程难以统一规范。而大规模预训练模型统一了“从 0 到 1”的技术路径，具备解决问题的泛化能力，能够赋能“从 1 到 100”的各类应用，并存在相对标准化的工作流，由此衍生出 AI Infra 投资机会。GPT-4 的开发经验也体现专业分工的必要性：根据 OpenAI 的披露，在 GPT-4 的开发过程中，其对 249 人研发团队进行了明确分工，并使用了数据标注、分布式计算框架、实验管理等点工具。

> 我们认为这也说明了在大模型时代应用基础软件的必要性。目前，AI Infra 产业处于高速增长的发展早期，我们预计未来几年内各细分赛道空间或保持 30%+ 的高速增长，且各方向均有变现实践与养成独角兽企业的潜力。

**“AI = Data + Code”**，组织 AI 所需的养料即数据，管理 AI 模型的训练部署过程，以及支持从模型到应用的整合是 AI Infra 工具的关键能力。

1. **数据准备**：无论是支持经典的机器学习模型还是大规模预训练模型，数据准备都是耗时较久、较为关键的一环。我们认为，LLM 浪潮下高质量的标注数据和特征库需求将持续增长，未来海量训练数据的需求或由合成数据满足。此外，我们强调 Data + AI 平台厂商的关键卡位。
2. **模型训练**：预训练模型的获取使得模型库更加流行，LLM 大规模训练需求也驱动底层分布式计算引擎和训练框架的迭代。此外，我们认为实验管理工具重要性较高。
3. **模型部署**：LLM 模型端的突破释放出大规模应用落地的潜能，更多模型从实验走向生产环境，我们认为有望整体提振模型部署和监控的需求。
4. **应用整合**：LLM 赋能应用催生对向量数据库和应用编排工具等的新需求。我们观察到经典的机器学习时代与大模型时代工具栈需求侧重点有所不同，同时，部分点工具正在拓宽产品功能边界，LLMOps 平台型产品的可及市场空间天花板或更高。

## AI Infra 是连接算力和应用的 AI 中间层基础设施

> 资料来源：Grand View Research，Foresight News，Gartner，MarketsandMarkets，拾象科技，Firstmark，a16z，各公司官网，中金公司研究部

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181315126.png)

本章主要讨论：

- AI Infra 在 AI 时代 IT 生态中的定位
- 为什么大模型浪潮下需要格外关注 AI Infra 投资机会
- AI Infra 基础软件工具栈涵盖内容
- AI Infra 商业化初探

### AI Infra 是 AI 时代的中间层基础设施

**从类比的角度理解 AI Infra：AI 时代连接硬件和上层应用的中间层基础设施**。

✅ 传统本地部署时代：三大基础软件（**数据库**、**操作系统**、**中间件**）实现控制硬件交互、存储管理数据、网络通信调度等共性功能，抽象并隔绝底层硬件系统的复杂性，让上层应用开发者能够专注于业务逻辑和应用功能本身的创新实现。

☁️ 云时代同理，形成了 `IaaS`、`PaaS`、`SaaS` 三层架构，其中 PaaS 层提供应用开发环境和基础的数据分析管理服务。

🤖 所以类比来看，我认为，进入 AI 时代也有承担类似功能的、连接算力和应用的基础设施中间层即 `AI Infra`，提供基础模型服务、赋能模型微调和应用开发。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181323671.png)

### LLM 催生 AI Infra 投资机会

LLM 流行前，AI 模型通用性较低，项目落地停留在“手工作坊”阶段，流程难以统一规范。人工智能已有数十年的发展历史，尤其是 2006 年以来以深度学习为代表的训练方法的成熟推动第三波发展浪潮。然而，**由于传统的机器学习模型没有泛化能力，大部分 AI 应用落地以定制化项目的形式，包括需求、数据、算法设计、训练评估、部署和运维等阶段，其中，数据和训练评估阶段往往需要多次循环，较难形成一套标准化的端到端的流程和解决方案，也由此造成了边际成本高、重复造轮子等问题**。

大规模预训练模型完成了“从 0 到 1”的技术统一，泛化能力和通用性释放出“从 1 到 100”的落地需求，且存在相对标准化的流程，衍生出 AI Infra 投资机会。基于 Transformer 算法、超大参数量的预训练模型拥有泛化能力，一定程度上解决了原先需要按项目定制训练的问题，过去正因为 ML 模型的非标和项目制，下游需求并未被完全激发出来，LLM 模型端的突破释放出更大规模的应用落地潜能。而后续的应用过程中主要涉及：高质量样本数据的准备、基础模型获取、模型微调及部署监控、应用编排开发上线等环节，工作流较为标准化，我们建议投资者持续关注 AI Infra 投资机会。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181327914.png)

### AI Infra 基础软件工具栈

参考海外 OpenAI 的率先尝试，工作流分工、点工具加持助力成功。一方面，OpenAI 在《GPT-4 Technical Report》论文中披露了参与 GPT-4 开发的人员分工，共 249 人，角色分工明确，预训练、强化学习和对齐、部署等 6 个大方向下又拆分成不同小组，其中数据集/数据基础设施、分布式训练基础设施、推理基础设施等分别对应工作流中的数据准备、模型训练、部署应用等环节；另一方面，OpenAI 使用了 Scale 数据标注服务、Ray 分布式计算框架和 Weights and Biases（W&B）实验管理工具，且 W&B 的创立灵感就来自于其创始人之一在 OpenAI 的实习经历。我们认为，OpenAI 的率先尝试经验一定程度上说明专业分工和 AI Infra 基础软件堆栈在大模型时代的必要性。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181329085.png)

**AI Infra 广义上包含了基础模型和基础软件栈两层，本篇报告核心关注其中和工作流相关的基础软件工具栈**。工作流的视角下，LLM 的开发应用主要涉及**数据准备**、**模型训练**、**模型部署**、**产品整合**四个主要环节，每个环节都有对应的点工具，亦有集大成的 LLMOps 平台型产品，我们将在下一章详细解读。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181331132.png)

### AI Infra 商业化

> 商业化起步中，已有变现实践，细分赛道或均有长出独角兽的潜力

商业化起步阶段，有望在未来几年快速成长为百亿美元量级的产业。我们认为，AI Infra 整体处于高速增长的发展早期，根据第三方数据，目前大部分细分赛道规模在几亿至几十亿美元量级，我们预计在未来几年内或将保持 30+% 的高速增长。同时，Data + AI、MLOps/LLMOps 等平台型产品的市场空间天花板可能更高，我们也观察到点工具厂商正在积极拓展产品边界。我们认为，AI Infra 是 AI 时代不可或缺的基础设施中间层，“掘金卖铲”逻辑的确定性高，有望持续受益于 LLM、AI 应用的繁荣。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181334064.png)

海外厂商积极探索变现，细分赛道或均有长出独角兽的潜力。从微观的视角，我们整理了 AI Infra 各细分赛道海外代表公司的商业模式，基本遵循按使用量付费的定价模式。大多数创业公司成立时间较短，目前收入体量在数千万至小几亿美元量级，其中数据相关的、平台型的厂商起步较早、已初具规模，我们认为这也符合数据需要前置于 AI 模型投入、平台型厂商收入天花板更高的逻辑。此外，我们认为 LLM 模型端突破将释放出更大规模应用落地的潜能，有望带动模型部署、应用整合等后续环节的逐步起量。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181335288.png)

## 从工作流视角梳理 AI Infra 投资机会 

### 大模型时代和传统机器学习时代工具栈侧重点有所不同

本章从企业训练模型、构建 AI 赋能应用的工作流视角出发，详解涉及的主要环节，并关注 LLMOps 和 MLOps 在流程上的侧重点差异。我们认为 AI = Data + Code，历经数据准备、模型训练、模型部署、产品整合，分环节看：

► **数据准备**：高质量标注数据、特征库需求持续，合成数据或成未来趋势。数据准备无论在传统的 MLOps 还是 LLMOps 中都是耗时较久、较为重要的一环。无监督学习降低对标注数据的需求，但 RLHF 机制体现了高质量标注数据的重要性，我们认为未来超大参数量模型对海量训练数据的需求或由合成数据满足。此外，Data + AI 平台厂商卡位关键。

► **模型训练**：模型库更加刚需，训练框架持续迭代，软件工具协助实验管理。基于通用的 LLM 大模型微调、蒸馏出小模型成为高性价比的落地方式，因此需要能够高效便捷地获取预训练模型的模型库；也催生更适应 LLM 大规模训练需求的底层分布式计算引擎和训练框架。此外，我们认为实验管理工具的重要性或始终较高。

► **模型部署**：更多模型从实验走向真实业务环境，部署和监控需求提升。我们认为，LLM 模型端的突破释放出大规模应用落地的潜能，更多的模型从实验环境走向生产环境，有望整体提振模型部署和监控的需求。

► **应用整合**：催生向量数据库和应用编排框架新需求。LLM 赋能应用催生出对应用产品整合相关工具产品的需求，其中较为关键的是向量数据库和应用编排工具。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181340897.png)

### 数据准备：高质量标注数据、特征库需求持续，合成数据或成未来趋势

**数据是模型的起点，一定程度上决定了模型的效果和质量，数据准备无论在传统的 MLOps 还是 LLMOps 中都是耗时较久、较为重要的一环**。LLM 带来的新变化主要包括：

1. 虽然 LLM 的无监督学习机制降低了对标注数据的需求，但 OpenAI 的 RLHF 体现了高质量标注数据重要性；
2. 模型规模大幅提升，带来日益增长的训练数据需求，长期看可能无法仅通过真实世界数据满足，合成数据提供一种 AIGC 反哺 AI 的解法。此外，数据基础管理软件平台的卡位始终关键，Data + AI 平台化趋势持续演进。

#### 数据标注

> GPT 的成功说明了高质量标注数据对提升模型效果的重要性

数据标注位于模型开发的最上游，对图像、视频、文本、音频等非结构化原始数据添加标签，为 AI 提供人类先验知识的输入。近年，无监督学习、强化学习等不需要标注数据的机器学习分支方法论的出现引发市场对于数据标注必要性的讨论与担忧。

不过，OpenAI 通过 RLHF 即基于人类反馈的强化学习来优化模型，且从 OpenAI 披露的分工中能看到有很多负责预训练、强化学习等的 AI 科学家也参与到数据准备中；开源的 [LLAMA 2](https://arxiv.org/abs/2307.09288) 的论文中也有一段强调高质量数据对模型训练结果影响的表述，Meta 与第三方供应商合作收集了近 3 万个高质量标注，又向市场证明了高质量数据标注工作的重要性。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181344886.png)

**数据标注厂商正在寻求智能化转型、减少对人力的依赖**。在数据标注助力 AI 快速发展的同时，AI 也将反哺数据标注更加自动化、智能化，如利用模型进行数据预处理再人工审核等。Meta AI 发布的 Segment Anything Model 的训练数据集 SA-1B，就是通过智能数据引擎来辅助自动化生成的，该数据引擎经历了辅助手动标注-半自动标注-自动化标注的训练过程。

#### 特征库

> 特征库（Feature Store）：高质量特征库持续受益

特征是预测模型的输入信号，可以简单理解为模型中的自变量 X，需要经过特征工程从原始数据中筛选得到。而特征库则是生产、管理、运营 ML 过程中所需数据及特征的系统，主要实现：

1. 运行各类数据管道（Pipeline）将原始数据转换为特征值；
2. 存储和管理特征和数据；
3. 为训练和推理提供一致的特征服务

目前该领域的代表性产品包括：开源项目如 Feast，独立商业化公司如 Tecton，大型科技厂商的 ML 平台如 Databricks、SageMaker 等中亦有相应模块。数据和特征的质量决定了机器学习的上限，我们认为高质量特征库有望持续受益，同时国内数据要素市场的蓬勃发展长期看有望为 AI 模型供应更多高质量的数据燃料。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181346863.png)

#### 合成数据

> 合成数据：做真实数据的“平替”，用 AIGC 反哺 AI

一项来自 Epoch AI Research 团队的研究预测存量的高质量语言数据将在 2026 年耗尽，低质量的语言和图像数据存量也将在未来的数十年间枯竭。面对潜在的数据瓶颈，合成数据即运用计算机模拟生成的人造数据，提供了一种成本低、具有多样性、规避了潜在隐私安全风险的解决方法，生成式 AI 的逐渐成熟进一步提供技术支撑。比如，自然语言修改图片的 Instruct-Pix2Pix 模型在训练的时候就用到 GPT-3 和 Stable Diffusion 来合成需要的提示词和图像的配对数据集；Amazon 也利用合成数据来训练智能助手 Alexa，以避免用户隐私问题。合成数据市场参与者较多，独立公司/项目如 gretel、MOSTLY AI、datagen、hazy 等，数据标注厂商如 Scale 亦推出相关产品，此外主流科技公司英伟达、微软、亚马逊等均有不同场景的尝试。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181350151.png)

#### Data + AI 是行业趋势

> 数据科学基础平台：数据卡位始终关键，Data + AI 是行业趋势

广义的数据科学涵盖利用各类工具、算法理解数据蕴藏含义的全过程，机器学习可以视为其中的一种方式和手段；狭义的数据科学也可以仅指代机器学习的前置步骤，包括准备、预处理数据并进行探索性分析等。

正如我们从报告[《人工智能十年展望（八）：探索 ChatGPT 根基——数据与人工智能如何相互成就？》](https://mp.weixin.qq.com/s?__biz=MzI3MDMzMjg0MA==&mid=2247626539&idx=2&sn=16f09dd8cc12354dbe8667b9de401bd3&chksm=eade09ecdda980faf65b9326f6ccaf2a92e197e241e6e5ac3ef1a042d67d901c81af112134a7&scene=21#wechat_redirect)开始一直强调的观点，数据和 AI 一体两翼，数据是模型的起点、且一定程度上决定了模型的最终效果和质量，数据基础设施厂商卡位关键，从 Data 向 AI 布局是技术能力和业务逻辑的自然延伸。LLM 等大模型的渗透发展不仅额外增加了数据平台上 AI 相关的工作流负载，还可以带动底层 Data 基础设施的需求。	

### 模型训练：模型库更加刚需，训练框架持续迭代，软件工具协助实验管理

大模型具有一定通用性，开发者们可以“站在巨人的肩膀上”，在预训练模型的基础上通过少量增量训练蒸馏出专精的小模型以解决垂类场景的需求。

LLM 带来的新变化主要包括：

1. 要想高效便捷地获取模型，则需要一个集成托管各类模型的社区也即模型库；
2. 催生更适应 LLM 大规模训练需求的底层分布式计算引擎和训练框架。

此外，模型训练过程涉及多次往复的修改迭代，无论是 ML 还是 LLM 都需要借助实验管理工具进行版本控制和协作管理。

#### 模型库

> 模型库（Model Hub）：把握从数据到模型的工作流入口

模型库顾名思义是一个**托管、共享了大量开源模型的平台社区**，供开发者下载各类预训练模型，除模型外，主流的 Model Hub 平台上还同时提供各类共享的数据集、应用程序 Demo 等，是 AI、ML 细分领域的“GitHub”。

典型代表厂商包括：

- 海外的 **Hugging Face**、Replicate
- 国内关注 Gitee（开源中国推出的代码托管平台）和 ModelScope（阿里达摩院推出的 AI 开源模型社区）等项目

在商业模型上，Model Hub 厂商一般选择切入下游的 AutoTrain（自动创建、优化、评估模型）或模型推理服务，也在尝试就 Model Hub 功能收取订阅制会员费用。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181357112.png)

#### 分布式计算和深度学习框架

> 分布式计算和深度学习框架：大模型“炼丹炉”

分布式计算引擎方面，LLM 的训练过程需要大规模的 GPU 分布式计算集群，过去大数据已带动了以 **MapReduce**、**Spark** 为代表的分布式计算引擎的发展，但以 Ray 为代表的近年在 AI 大潮下兴起的分布式计算框架则更贴合 AI 需求（Ray 的首篇论文名为[《Ray: A Distributed Framework for Emerging AI Applications》](https://www.usenix.org/system/files/osdi18-moritz.pdf)），其核心模块 Ray Tune、Ray Rllib、Ray Train 分别对应机器学习调参、强化、深度学习调参的流程。Ray 在官网的用户案例中表示“Ray 是使 OpenAI 能够增强其训练 ChatGPT 和类似模型能力的关键”。此外，Ray 作为更底层的分布式计算引擎，和 TensorFlow、PyTorch 等深度学习框架兼容，而 DeepSpeed、ColossalAI 等则是在 PyTorch 等基础框架之上针对 LLM 的优化训练设计的新一代框架。

#### 实验管理

> 实验管理：记录实验元数据，辅助版本控制，保障结果可复现

模型训练是一种实验科学，需要反复的修改与迭代，同时由于无法提前预知实验结果往往还涉及版本回溯、多次往复，因此模型的版本控制和管理就较为必要，实验管理软件可以辅助技术人员和团队追踪模型版本、检验模型性能。该领域代表厂商为 Weights and Biases（W&B）和 Neptune，跟踪机器学习实验，记录实验元数据，包括训练使用数据集、框架、进度、结果等，支持以可视化的形式展现结果、多实验结果对比、团队协作共享等。此外，实验管理也是 LLMOps/MLOps 平台型产品如星环科技 Sophon、Google Vertex AI 等产品中的重要模块之一。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181400486.png)

### 模型部署：更多模型从实验走向真实业务环境，部署和监控需求提升

模型部署是让模型从实验环境走向真实生产环境的重要环节，借助**模型部署**工具能够解决模型框架兼容性差的问题并提升模型运行速度。

**模型监控**通过对模型输出结果和性能指标的追踪，保障模型上线后的可用性。

我们认为，过去由于 ML 模型的非标和项目制，大规模、持续性的模型部署和监控需求未被完全激发出来，LLM 模型端的突破释放出大规模应用落地的潜能，更多的模型从实验环境走向生产环境，我们认为有望整体提振模型部署和监控的需求。

#### 模型部署

> 模型部署：从实验走向生产的重要环节

模型部署指把训练好的模型在特定环境中运行，需要尽量最大化资源利用效率，保证用户使用端的高性能。模型部署领域参与者较多，比如 Ray、Tensorflow、PyTorch 等训练框架都提供配套的模型部署功能，模型库厂商如 Hugging Face、实验管理厂商如 W&B 也有相关产品，此外还有如 Seldon、BentoML、OctoML 等独立项目/产品。和训练框架自带的部署模块相比，三方的综合性产品能够为不同框架下训练出来的模型提供一套相对统一的部署方式。以 Seldon 为例，在复杂的多模型推理场景下，Seldon 通过模型可解释性、异常值检测等模块，最终选出表现最好的模型进行结果反馈。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181402930.png)

#### 模型监控

> 模型监控：模型可观测性保障可靠可用

可观测性在传统 IT 系统运维中就是重要的数智化手段之一，通过监控各类机器、系统的运行数据对故障和异常值提前告警。模型监控同理，监测模型上线后的数据流质量以及表现性能，关注模型可解释性，对故障进行根因分析，**预防数据漂移、模型幻觉等问题**。

模型可观测性领域有较多创业公司，包括 Fiddler、WhyLabs、Evidently AI 等，实验管理厂商如 W&B、模型部署厂商如 Seldon 也有所涉及，此外，传统的 IT 运维可观测性厂商也有机会切入 AI 模型监控领域，海外如 Datadog 已经尝试将 Open AI 的模型服务加入纳管范畴，我们也建议关注国内相关厂商的后续进展。

### 应用整合：催生向量数据库和应用编排框架新需求

正如前文提及，LLM 模型端的突破释放出更多应用落地的潜能，由此**催生出对应用产品整合相关工具产品的需求，其中较为关键的是向量数据库和 LLM 应用编排工具**。

#### [向量数据库](/documents/academic/向量数据库·科普.pptx)

> 向量数据库：LLM 的外部知识库

让通用大模型具备专业知识主要有两种途径，一是通过微调将专有知识内化到 LLM 中；另一种则是利用向量数据库给 LLM 增加外部知识库，后者成本更低。

向量数据库和 LLM 的具体交互过程为：用户首先将企业知识库的全量信息通过嵌入模型转化为向量后储存在向量数据库中，用户输入 `prompt` 时，先将其同样向量化，并在向量数据库中检索最为相关的内容，再将检索到的相关信息和初始 `prompt` 一起输入给 LLM 模型，以得到最终返回结果。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181410989.png)

向量化技术本身已较为成熟，海外模型如 **Word2Vec**、FastText 等，国内中文 Embedding 模型有 MokaAI 开源的 M3E、IDEA CCNL 开源的二郎神系列。向量数据库厂商/产品主要包括 **Pinecone**、**Zilliz**、星环科技 Hippo 等，另外也有传统数据库、大数据平台厂商如 PGSQL、Databricks 通过**增加向量查询引擎插件来实现支持**。我们认为，向量数据库是 AI Answers 类应用落地的刚需，同时本土厂商在中文 Embedding 方面可能更具优势。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181418267.png)

#### 应用编排框架

> 应用编排框架：LLM 应用“粘合剂”

LLM 应用编排框架是一个封装了各种大语言模型应用开发所需逻辑和工具的代码库，LangChain 是当下最流行的框架之一，还有 Anarchy、Dust、AutoGPT、LlamaIndex 等。初始化的大模型存在无法联网、无法调用其他 API、无法访问本地文件、对 Prompt 要求高、生成能力强但内容准确度无法保证等问题，应用编排框架提供了相应功能模块，帮助实现从 LLM 到最终应用的跨越。

以 LangChain 为例，它主要包含以下几个模块：

1. Prompt 实现指令的补全和优化；
2. Chain 调用外部数据源、工具链；
3. Agent 优化模块间的调用顺序和流程；
4. Memory 增加上下文记忆。

#### 集成开发环境

> 集成开发环境：交互式 Notebook 逐渐流行

在上述 AI 建模流程中，开发者需要处理大量代码编写、分析、编译、调试等工作，可以直接在对应环节或平台型产品的内置环境中进行，也可以使用专门的集成开发环境并调取所需功能。其中，Notebook 是一种交互式的开发环境，和传统的非交互式开发环境相比，Notebook 可以逐单元格（Cell）编写和运行程序，出现错误时，仅需调整并运行出现错误的单元格，大大提升开发效率，因此近年逐渐流行、深受数据科学家和算法工程师的喜爱，被广泛应用于 AI 算法开发训练领域。

### LLMOps 一站式解决方案或更适应国内市场

前文我们详细介绍了模型训练、构建应用工作流涉及的主要环节及各环节点工具厂商，事实上，**这些厂商在强项环节之外亦不断拓宽产品能力边界**，比如：

- 数据标注厂商 Scale AI 拓展合成数据业务并正在投入 LLMOps 领域的 Scale Spellbook（做一个基于大语言模型的开发者工具平台）；
- 模型库厂商 Hugging face 切入 AutoTrain 和模型部署；
- 实验管理厂商 W&B 切入模型部署和模型监控等。

**MLOps/LLMOps 提供一站式平台解决方案，可及市场空间更大，多采取 Data + AI 一体化战略**。除此之外还有平台型的 MLOps/LLMOps 产品，基本涵盖了上述流程的主要环节，大型科技企业、数据基础软件厂商均参与其中。

我们认为，基于整体数字化进程和软件付费意愿习惯判断，海外企业客户可能倾向于选取各环节点工具自组工具栈，而国内客户可能倾向于一站式的解决方案。此外，从目前 AI Infra 领域独角兽的估值水平来看，平台型厂商多采取 Data + AI 一体化战略，起步较早、规模天花板更高。

![](https://cdn.jsdelivr.net/gh/Wu-yikun/OSS/PicGo/202507181416723.png)
